---
layout: default
title: Mein Portfolio
font_family: "Montserrat"
text_align: justify
lang: de
---

# <i class="fa fa-trophy"></i> Mein Portfolio <!-- omit from toc -->

_Eine Auswahl von Projekten, die meine Expertise in der Entwicklung und Anwendung wirkungsvoller KI-L√∂sungen f√ºr industrielle Herausforderungen demonstrieren. Diese Beispiele verdeutlichen meine F√§higkeit, **Softwareprodukte f√ºr die vorausschauende Instandhaltung (PdM) f√ºr Technologieanbieter zu optimieren und strategische, datengest√ºtzte Erkenntnisse zu liefern, die f√ºr Anlagenbetreiber handlungsrelevant sind.**_

## <i class="fa fa-list-ul"></i> √úberblick <!-- omit from toc -->

- [Weiterentwicklung von Anomalieerkennungs-Funktionen](#anomaly-detection-product)

- [Verbesserung von Genauigkeit & Effizienz digitaler Zwillinge](#gas-turbine-digital-twin)

- [Industrialisierung der Datenaufnahme](#data-ingestion-industrialization)

- [Entwicklung eines KI-gest√ºtzten Q&A-Assistenten](#genai-qna-companion)

- [Fortgeschrittene Datenanalyse & Modellierung](#scientific-data-analysis)

## <i class="fa fa-check-square"></i> Projekte

{:#anomaly-detection-product}
<br>

### <i class="fa fa-bolt"></i> Weiterentwicklung von Anomalieerkennungs-Funktionen f√ºr ein neues PdM-Produkt

<div style="text-align: center;">
<img src="{{ site.url_ai_images }}/ChatGPT Image May 14, 2025, 07_55_14 PM - Advancing Early Anomaly Detection Capabilities for a New PdM SaaS Product.svg" alt="Weiterentwicklung von Anomalieerkennungs-Funktionen f√ºr ein neues PdM-SaaS-Produkt" height="200"/>
</div>

_F√ºr einen f√ºhrenden SaaS-Anbieter von PdM-Produkten umfasste dieses Projekt die Leitung der Algorithmenentwicklung f√ºr ein neues Produkt zur Fr√ºherkennung von Anomalien. Der Fokus lag auf der Erstellung hochentwickelter, zuverl√§ssiger und interpretierbarer Modelle f√ºr Industrieanlagen._

- **Herausforderung:** Der SaaS-Anbieter beabsichtigte, ein neues Flaggschiff-PdM-Produkt zu entwickeln. Dieses sollte in der Lage sein, subtile Anomalien in Industrieanlagen in einem fr√ºhen Stadium zu erkennen und √ºber einfache, regelbasierte Warnmeldungen hinauszugehen, um anspruchsvollere und zuverl√§ssigere pr√§diktive Einblicke zu erm√∂glichen.
- **L√∂sung:** Ich konzipierte, benchmarkte und implementierte eine Reihe fortschrittlicher Algorithmen zur Anomalieerkennung, die sowohl kausale als auch nicht-kausale Methoden (z.B. AAKR, Isolation Forest, PCA-basiertes Clustering) umfassten. Ein wesentlicher Bestandteil der L√∂sung war die Entwicklung eines Frameworks zur Injektion simulierter Fehler (Mock-Faults) f√ºr eine robuste Validierung. Vollst√§ndige MLOps-Pipelines wurden f√ºr effizientes Training, Deployment (auf Kubernetes) und Inferenz etabliert.
- **Ergebnis:** Deutliche Verbesserung der Genauigkeit bei der Anomalieerkennung und der Interpretierbarkeit der Modelle, was zu zuverl√§ssigeren und handlungsrelevanteren Fr√ºhwarnungen f√ºr Endanwender f√ºhrte. Die optimierten MLOps-Prozesse erm√∂glichten ein effizientes Deployment und schnelle Updates der Modelle. Diese Arbeit war **entscheidend f√ºr die erfolgreiche Markteinf√ºhrung des neuen SaaS-Produkts zur Fr√ºherkennung von Anomalien und trug ma√ügeblich zur Verdoppelung des Produktportfolios des Unternehmens bei.**
- **Wichtige Erkenntnisse f√ºr Anlagenbetreiber:** Dieses Projekt unterstrich die Bedeutung robuster MLOps und der Validierung mittels simulierter Fehler f√ºr die Entwicklung vertrauensw√ºrdiger KI in kritischen Systemen ‚Äì Schl√ºsselaspekte f√ºr jede Organisation bei der Auswahl oder Implementierung von PdM-L√∂sungen.
- **Technische Highlights / Verwendete Technologien:** Python, Algorithmen zur Anomalieerkennung (AAKR, Isolation Forest, Clustering, kausale Methoden), MLOps (Kubernetes, NATS, GitHub Actions), Modellerkl√§rbarkeit (Shapley Values), Generierung & Test von simulierten Fehlern, Frameworks f√ºr Performance-Benchmarking.

{:#gas-turbine-digital-twin}
<br>

### <i class="fa fa-cogs"></i> Verbesserung von Genauigkeit & Effizienz digitaler Zwillinge von Gasturbinen durch statistische Modellierung

<div style="text-align: center;">
<img src="{{ site.url_ai_images }}/ChatGPT Image May 14, 2025, 07_51_31 PM - Enhancing Gas Turbine Digital Twin Accuracy & Efficiency with Statistical Modeling.svg" alt="Verbesserung von Genauigkeit & Effizienz digitaler Zwillinge von Gasturbinen" height="200"/>
</div>

_F√ºr dasselbe SaaS-Startup zielte dieses Projekt darauf ab, die Genauigkeit der digitalen Zwillingsmodelle f√ºr Gas-und-Dampf-Kombikraftwerke (GuD) zu verbessern und den Kalibrierungsaufwand durch die Integration fortschrittlicher statistischer Methoden zu reduzieren._

- **Herausforderung:** Der digitale Zwilling des Unternehmens f√ºr die Ursachenanalyse von Fehlern (Root-Cause Failure Analysis) in GuD-Anlagen, der prim√§r auf physikalischen Modellen basierte, litt unter langen Kalibrierungszeiten und einer begrenzten Sensitivit√§t bei der Fehlererkennung. Es bestand die Notwendigkeit, die Modellgenauigkeit und -effizienz ohne umfangreiche manuelle Anpassungen zu steigern.
- **L√∂sung:** Ich entwickelte und benchmarkte fortschrittliche statistische Modelle (Python, Scikit-learn), die die bestehenden physikalischen digitalen Zwillinge erg√§nzten. Dies umfasste die Implementierung einer neuartigen "Best-Performance"-Datenfilterungstechnik, robustes Feature Engineering aus Sensordaten sowie die Leitung einer rigorosen Vergleichsanalyse mit den traditionellen physikalischen Modellen.
- **Ergebnis:** Erfolgreiche Demonstration einer **Reduktion der Modellkalibrierungszeiten und Anpassungskosten um √ºber 50%** im Vergleich zu rein traditionellen Ans√§tzen, bei gleichzeitiger Verbesserung der diagnostischen Genauigkeit und Fehlererkennungssensitivit√§t f√ºr zentrale Turbinenparameter. Die neuen datengesteuerten Methoden und Filter wurden √ºbernommen und kamen sowohl statistischen als auch physikalisch-basierten Modellworkflows zugute.
- **Wichtige Erkenntnisse f√ºr Anlagenbetreiber:** Der Erfolg der Integration statistischer Modelle mit physikbasierten digitalen Zwillingen zeigt einen pragmatischen Ansatz zur Steigerung von Genauigkeit und Effizienz ‚Äì eine wertvolle Strategie, um mehr aus bestehenden Investitionen in digitale Zwillinge herauszuholen oder neue zu planen.
- **Technische Highlights / Verwendete Technologien:** Python, Scikit-learn, fortschrittliche statistische Modellierung (Regression), Zeitreihenanalyse, Feature Engineering, benutzerdefinierte Daten-Clusterbildung & Binning, Modell-Benchmarking, "Best-Performance"-Datenfilterlogik.

{:#data-ingestion-industrialization}
<br>

### <i class="fa fa-database"></i> Industrialisierung der Datenaufnahme f√ºr einen SaaS-Anbieter im Bereich Predictive Maintenance

<div style="text-align: center;">
<img src="{{ site.url_ai_images }}/ChatGPT Image May 15, 2025, 04_16_23 PM - Robust Data Engineering & ETL Pipelines for Industrial Data.svg" alt="Industrialisierung der Datenaufnahme f√ºr Predictive Maintenance" height="200"/>
</div>

_F√ºr denselben SaaS-Anbieter konzentrierte sich dieses Projekt auf die Erstellung einer robusten, skalierbaren und standardisierten ETL-Pipeline zur Verarbeitung vielf√§ltiger und komplexer Daten von Industriekunden._

- **Herausforderung:** Der Anbieter sah sich mit erheblichen Verz√∂gerungen und hohen Nacharbeitskosten bei der Kalibrierung seines Kernprodukts f√ºr Predictive Maintenance konfrontiert. Urs√§chlich hierf√ºr waren inkonsistente, fehlerbehaftete und unterschiedlich formatierte Daten von neuen Industriekunden, was eine effiziente Produktbereitstellung und Skalierbarkeit des Teams behinderte.
- **L√∂sung:** Ich konzipierte und entwickelte eine universelle Python-basierte ETL-Pipeline. Zu den Kernfunktionen geh√∂rten adaptives Parsen verschiedener Dateitypen (CSV, mehrseitige Excel-Tabellen), robuste Zeitstempel-/Zeitzonenkorrektur, automatische Bereinigung von Tag-Namen, blockbasierte Verarbeitung gro√üer Datenmengen und eine umfassende Fehlerbehandlung. Die L√∂sung umfasste die Integration von CI/CD (GitHub Actions), umfangreiche Dokumentation und Teamschulungen zur breiten Akzeptanz.
- **Ergebnis:** Erzielung einer **Reduktion der Datenverarbeitungszeit um √ºber 90%**, was die Kundenintegration und Produktkalibrierung erheblich beschleunigte. Dies minimierte Projektrisiken und Nacharbeiten, verbesserte die allgemeine Datenqualit√§t und erm√∂glichte einen skalierbaren, standardisierten Datenaufnahmeprozess, der vom gesamten Data-Science-Team √ºbernommen wurde.
- **Wichtige Erkenntnisse f√ºr Anlagenbetreiber:** Diese Initiative unterstreicht die kritische Rolle standardisierter, robuster Datenaufnahmepipelines bei jeder erfolgreichen KI- oder PdM-Implementierung. Die Sicherstellung von Datenqualit√§t und -zug√§nglichkeit von Beginn an ist entscheidend f√ºr zuverl√§ssige Analyseergebnisse.
- **Technische Highlights / Verwendete Technologien:** Python, Pandas, benutzerdefinierte ETL-Architektur, fortschrittliches Parsen von Dateien (CSV/Excel), Zeitstempel-/Zeitzonenkorrektur, Datenvalidierung & -bereinigung, CI/CD (GitHub Actions), blockbasierte Verarbeitung f√ºr Big Data, umfassende Dokumentation.

{:#genai-qna-companion}
<br>

### <i class="fa fa-comments-o"></i> Entwicklung eines KI-gest√ºtzten Q&A-Assistenten mit Generativer KI

<div style="text-align: center;">
<img src="{{ site.url_ai_images }}/ChatGPT Image May 14, 2025, 08_12_56 PM - Developing an AI-Powered Q&A Companion with Generative AI.svg" alt="Entwicklung eines KI-gest√ºtzten Q&A-Assistenten mit Generativer KI" height="200"/>
</div>

_Dieses pers√∂nliche Projekt wurde initiiert, um meine Expertise im Bereich Generative KI durch die Entwicklung eines End-to-End Retrieval-Augmented Generation (RAG)-Systems zu vertiefen. Ziel war es, eine umfangreiche √∂ffentliche Bibliothek wissenschaftlich fundierter Blog-Inhalte (NutritionFacts.org) in eine interaktive, dialogorientierte Wissensquelle umzuwandeln und so eine praxisnahe und aussagekr√§ftige Anwendung zur Demonstration fortgeschrittener GenKI-F√§higkeiten zu schaffen._

- **Herausforderung (Selbstgestecktes Ziel):** Die Erforschung und praktische Beherrschung der Erstellung eines komplexen RAG-Systems, das eine gro√üe (1200+ Beitr√§ge), unstrukturierte, dom√§nenspezifische Wissensdatenbank durch nat√ºrlichsprachliche Anfragen zug√§nglich und interaktiv macht.
- **L√∂sung (Meine Initiative):** Ich konzipierte und implementierte eigenst√§ndig "Nutrify Your Life", einen voll funktionsf√§higen RAG-basierten Q&A-Chatbot. Dies umfasste:
  - Systematisches Scrapen und Verarbeiten der √∂ffentlichen Blog-Inhalte.
  - Aufbau einer Wissensdatenbank mittels Sentence Embeddings und einer LanceDB Vektordatenbank.
  - Implementierung einer fortschrittlichen Retrieval-Pipeline mit hybrider Suche (Vektor + Keyword) und Cross-Encoder Reranking.
  - Integration mit LLMs (√ºber Groq API) zur pr√§zisen, kontextbezogenen Antwortgenerierung mit Quellenangabe.
  - Entwicklung einer benutzerfreundlichen Streamlit-Oberfl√§che und eines MongoDB-basierten Monitoring-Dashboards.
- **Ergebnis:** Erfolgreiche Entwicklung und Bereitstellung einer voll funktionsf√§higen KI-Anwendung, die statische Inhalte in ein dynamisches, interaktives KI-Asset umwandelt und Nutzern unmittelbare und zuverl√§ssige Antworten liefert. Dieses Projekt festigte meine praktischen F√§higkeiten im gesamten Entwicklungszyklus von GenKI-L√∂sungen. _Die demonstrierten RAG-Methoden und die Architektur sind direkt auf die Erstellung √§hnlicher Systeme f√ºr industrielle technische Dokumentationen, Wartungsprotokolle oder Compliance-Richtlinien anwendbar._

  üëâ <a href="https://nutrify-your-life.streamlit.app/" target="_blank">Live-Demo</a>

- **Technische Highlights / Verwendete Technologien:** Generative KI (RAG), Python, Streamlit, LLM APIs (Groq), Sentence Transformers, Vektordatenbanken (LanceDB), Hybride Suche, Cross-Encoder Reranking, Prompt Engineering, Data Scraping/Verarbeitung, MongoDB, Docker.

  <a href="https://github.com/alexkolo/rag_nutrition_facts_blog" target="_blank">GitHub-Repository</a>

{:#scientific-data-analysis}
<br>

### <i class="fa fa-star-o"></i> Fortgeschrittene Datenanalyse & Modellierung komplexer wissenschaftlicher Datens√§tze

<div style="text-align: center;">
<img src="{{ site.url_ai_images }}/cosmic_data_analysis.svg" alt="Abstrakte Darstellung komplexer Datenanalyse kosmischer Datens√§tze" height="200"/>
</div>

_Basierend auf jahrelanger Erfahrung als leitender Forscher in der Astrophysik (Max-Planck-Institut, Kavli-Institut, CNRS/Universit√§t Paris-Saclay) spezialisierte ich mich auf die Extraktion schwacher Signale und aussagekr√§ftiger Erkenntnisse aus petabytegro√üen, verrauschten Datens√§tzen von weltraumgest√ºtzten Observatorien (z.B. XMM-Newton, Chandra, ROSAT)._

- **Herausforderung:** Die Beantwortung fundamentaler kosmologischer Fragen erforderte innovative Methoden zur Analyse von Daten mit extrem niedrigem Signal-Rausch-Verh√§ltnis, zur Modellierung komplexer physikalischer Ph√§nomene und zur Unterscheidung zwischen instrumentellen Effekten und echten astrophysikalischen Signalen in riesigen Datens√§tzen.
- **L√∂sung & Demonstrierte Expertise:**
  - **Anspruchsvolle Signalverarbeitung & Hintergrundmodellierung:** Entwicklung und Anwendung fortschrittlicher Techniken zur Zeitreihenanalyse, Ausrei√üerfilterung und komplexen Hintergrundsubtraktion zur Isolierung schwacher Emissionen.
  - **Fortgeschrittene statistische Modellierung:** Einsatz von Bayes'scher Inferenz (MCMC), Maximum-Likelihood-Sch√§tzung und ma√ügeschneiderten Python-Paketen zur Modellierung mehrdimensionaler r√§umlicher und spektraler Daten zur robusten Sch√§tzung physikalischer Parameter kosmischer Strukturen.
  - **Entwicklung von ETL-Pipelines:** Konzeption und Implementierung automatisierter, wiederverwendbarer Python- und Shell-Skript-basierter ETL-Pipelines zur Verarbeitung und Kalibrierung gro√üer Mengen Beobachtungsdaten, um Konsistenz und Effizienz f√ºr Forschungsteams sicherzustellen.
  - **Entwicklung innovativer Methoden:** Pionierarbeit bei neuen Analysemethoden f√ºr die Untersuchung von Oberfl√§chenhelligkeitsschwankungen, die neuartige Diagnosewerkzeuge f√ºr die Untersuchung des Intracluster-Mediums und gro√ür√§umiger kosmischer Strukturen bereitstellten.
  - **Umgang mit Big Data:** Routinem√§√üige Verwaltung und Analyse von Terabytes an Daten, Entwicklung skalierbarer Analysewerkzeuge und Workflows.
- **Ergebnis & √úbertragbare F√§higkeiten:**
  - Verfassen mehrerer Erstautor-Publikationen in f√ºhrenden, begutachteten Fachzeitschriften (z.B. _Astronomy & Astrophysics_) und Einwerbung kompetitiver Forschungsgelder.
  - **Wichtige √ºbertragbare Expertise:** Die rigorosen Analysetechniken, die statistische Tiefe und die Erfahrung im Umgang mit komplexen, hochvolumigen, verrauschten Daten sind direkt auf die Bew√§ltigung anspruchsvoller industrieller Datens√§tze anwendbar. Dies umfasst robustes Feature Engineering, fortschrittliche Anomalieerkennung in Umgebungen mit niedrigem Signal-Rausch-Verh√§ltnis und den Aufbau zuverl√§ssiger Datenverarbeitungspipelines ‚Äì unerl√§sslich sowohl f√ºr die Entwicklung anspruchsvoller PdM-Software als auch f√ºr die Beratung von Endanwendern hinsichtlich effektiver Datenstrategien.
- **Technische Highlights / Verwendete Technologien:** Python (NumPy, SciPy, Astropy), Fortgeschrittene Statistische Inferenz (Bayes/MCMC), Zeitreihenanalyse, Bildverarbeitung, ETL-Pipeline-Entwicklung, Big-Data-Analytik, Wissenschaftliches Programmieren, Hochleistungsrechnen (HPC)-Umgebungen.
